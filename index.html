<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Master's Thesis - MIREX 2015 submission</title>

		<meta name="description" content="Trabajo de Fin de Máster - MUIARFID">
		<meta name="author" content="Leopoldo Pla Sempere">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/sky.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Master's Thesis</h1>
					<h3>MIREX 2015 submission</h3>
					<p>
						<small><p>Author: <a href="mailto:leoplsem@posgrado.upv.es">Leopoldo Pla Sempere</a> / Supervised by: <a href="mailto:rparedes@dsic.upv.es">Roberto Paredes Palacios</a></p>
						<p><a href="http://www.upv.es/titulaciones/MUIARFID/">Master's Degree in Artificial Intelligence, Pattern Recognition and Digital Imaging 2014-2015</a> / <a href="http://www.upv.es">Polytechnic University of Valencia</a></p></small>
					</p>
					<aside class="notes"></aside>
				</section>

				<section>
					<h2>Presentation structure</h2>
					<ul class="fragment roll-in">
						<li>Introduction</li>
						<li>Implementation</li>
						<li>Experiments</li>
						<li>Conclusions</li>
					</ul>
				</section>
				<section>
					<section>
						<h2>Introduction</h2>
					</section>
					<section>
					  <h2>Main idea</h2>
						<ul>
							<li class="fragment roll-in">Many Music Information Retrieval fields use timbral and chroma features of the audio signal</li>
						  <li class="fragment roll-in">In computational linguistics authorship attribution is usual to use features from the text representation</li>
							<li class="fragment roll-in">This project is based on the idea of musical structure analysis of classical audio files to identificate composers.</li>
					</section>
					<section>
						<h3>MIREX</h3>
						<div align="left" class="fragment">Music Information Retrieval Evaluation eXchange (MIREX)</div>
						<ul>
							<li class="fragment roll-in">organized by The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana-Champaign (UIUC)</li>
							<li class="fragment roll-in">hold as part of the 16th International Conference on Music Information Retrieval, ISMIR 2015 (Malaga)</li>
							<ul>
								<li class="fragment roll-in">Audio Classification (Train/Test) Tasks</li>
								<ul>
									<li class="fragment roll-in">Audio Classical Composer Identification</li>
								</ul>
							</ul>
						</ul>
						<aside class="notes">
						</aside>
					</section>

					<section>
						<h3>Theoretical framework</h3>
						<ul>
							<li class="fragment roll-in">Machine Learning</li>
						  <li class="fragment roll-in">Music Theory</li>
						</ul>
					</section>

					<section>
						<h3>Machine Learning</h3>
						<img src="img/machinelearning.png" alt="ML">

						<aside class="notes">
							Hablamos del machine learning (y más concretamente del pattern recognition) como la técnica por la que los sistemas de computación son capaces de reconocer un entorno adquiriendo información a través de sensores
							En base a los paradigmas de clasificación, reconocerlos implica clasificarlos en clases dentro de un conjunto C con la mínima probabilidad de error.
						</aside>
					</section>
					<section>
						<h2>Neural Network</h3>
						<img src="img/neuralNet.png" alt="NN" style="width: 70%; height: 70%">

						<aside class="notes">
							Las redes son un tipo de algoritmo de clasificación que pertenece a los modelos conectionistas y son entrenadas para resolver problemas aprendiendo las características del problema utilizando información clasificada (superviced learning)
						</aside>
					</section>
					<section>
						<h2>Deep Belief Network</h3>
						<img src="img/dbn_model.png" alt="DBN" style="width: 70%; height: 70%">

						<aside class="notes">
							The Deep Belief Network is a type of Deep Learning algorithms, which is
							based on a set of stacked Restricted Boltzmann Machines undirectly connec-
							ted and trained with unsupervised learning, usually the Contrastive Diver-
							gence algorithm. These RBMs try to reconstruct the data layer by layer.
							1.4 The resultant DBN can be unfolded into a Multi Layer Perceptron (feed
							forward network) with initialized values and after it can be "fine-tuned" with
							labeled data using backpropagation.
						</aside>
					</section>
					<section>
						<h3>Musical Analysis</h3>
						<ul class="fragment roll-in">
						<li>The process and the academic discipline which studies the musical works from the pattern aspect, the internal structure, composition techniques and interpretative aspects.</li>
						</ul>
						<ul class="fragment roll-in">
							<li>Structure</li>
							<li>Melody</li>
							<li>Texture</li>
							<li>Harmony</li>
							<li>Tone</li>
							<li>Dynamics</li>
						</ul>
					</section>
					<section>
						<h2>Example of written musical analysis</h3>
						<img src="img/musical_analysis.jpg" alt="AnalysisExample" style="width: 55%; height: 55%">
					</section>

					<section>
						<h2>State of the art</h2>
						<p>
							<ul>
								<li class="fragment">Previous submissions of composer identification in MIREX</li>
								<ul>
									<li class="fragment"> Classifiers:</li>
									<ul>
									  <li class="fragment">Support Vector Machines</li>
										<li class="fragment">Convolutional Neural Networks</li>
										<li class="fragment">Restricted Boltzmann Machines</li>
										<li class="fragment">Gaussian Mixture Models</li>
										<li class="fragment">k-Nearest Neighbours</li>
										<li class="fragment">Multi Layer Perceptron</li>
								  </ul>
								</ul>
								<ul>
									<li class="fragment"> Features:</li>
									<ul>
										<li class="fragment">Mel-Frequency Cepstral Coefficients</li>
										<li class="fragment">Decorrelated Filter Banks</li>
										<li class="fragment">Octave-based Spectral Contrast</li>
										<li class="fragment">Spectral Patterns</li>
										<li class="fragment">Short Time Fourier Transform</li>
										<li class="fragment">Logarithmic Fluctuation Patterns</li>
								  </ul>
								</ul>
							</ul>
						</p>


						<aside class="notes">

						</aside>
					</section>
				</section>

				<section>
					<section data-background="#8ED5BA" data-background-transition="slide">
					<h2>Implementation</h2>
					</section>
					<section data-background="#8ED5BA" data-background-transition="slide">
						<h2>Workflow diagram</h3>
						<img src="img/diagram.png" alt="SoftwareDiagram" style="width: 30%; height: 30%">
					</section>
					<section data-background="#8ED5BA" data-background-transition="slide">
						<h2>Features</h3>
						<ul>
							<li class="fragment">Key Mode</li>
							<ul><li class="fragment">Queen Mary Vamp Plugin</li></ul>
							<li class="fragment">Segmentation</li>
							<ul><li class="fragment">Queen Mary Vamp Plugin</li></ul>
							<li class="fragment">Tonality</li>
							<ul><li class="fragment"><img src="img/formula.png" alt="Formula" style="width: 85%; height: 85%"></li></ul>
							<li class="fragment">Harmonic analysis</li>
							<li class="fragment">MFCC Means</li>
							<ul><li class="fragment">Queen Mary Vamp Plugin</li></ul>
						</ul>
						<aside class="notes">
							The main work has been done in these following high level features based
							on the key of the sample and the detected chords. First of all, I obtain
							through the key strength Vamp plugin the value between -1 and 1 of
							every key (from C, C#, and so on, to B minor and major) of every window
							of 1 chroma frame. Then, I select the most strong key of every window (in
							the figure, the most red value of a column) obtaining a vector of keys.

							From this keys vector, I created a script in Python 2.4 that obtains the
							key of the sample using a weighted sum of the number of perfect cadences
							found at the key strength using 1 chroma and the values of key strength
							plugin using 10 chroma. In the mathematical notation, keys are sorted by 12
							Major tones and then 12 minor tones, the ponderation in the implementation
							is 12 after some tests and the w is the 1 and 10 chroma window through the
							algorithm iterates. This obtained key is used as a feature.
						</aside>
					</section>
					<section data-background="#8ED5BA" data-background-transition="slide">
						<h2>Classifier</h3>
						<ul>
							<li class="fragment roll-in">Multi Layer Perceptron (MLP)</li>
							<li class="fragment roll-in">Deep Belief Network (DBN)</li>
							<ul>
								<li class="fragment roll-in">z-score normalization</li>
								<li class="fragment roll-in">Normalization between 0 and 1</li>
								<li class="fragment roll-in">44 neurons at hidden layer</li>
								<li class="fragment roll-in">300 epochs</li>
								<li class="fragment roll-in">Dynamic setting of batch size</li>
								<li class="fragment roll-in">Sigmoid activation function</li>
								<li class="fragment roll-in">Softmax output function</li>
							</ul>
						</ul>
						<aside class="notes">

						</aside>
					</section>
				</section>
				<section>
					<section data-background="#E1D9FC" data-background-transition="slide">
						<h2>Results</h2>
					</section>
					<section data-background="#E1D9FC" data-background-transition="slide">
						<h3>Dataset</h3>
						<ul>
							<li class="fragment">11 "classical" composers</li>
							<ul class="fragment roll-in">
								<li>Bach</li>
								<li>Beethoven</li>
								<li>Brahms</li>
								<li>Chopin</li>
								<li>Dvorak</li>
								<li>Handel</li>
								<li>Haydn</li>
								<li>Mendelssohn</li>
								<li>Mozart</li>
								<li>Schubert</li>
								<li>Vivaldi</li>
							</ul>
						</ul>
					</section>
					<section data-background="#E1D9FC" data-background-transition="slide">
						<h3>Dataset</h3>
						<ul>
							<li class="fragment">Homemade database</li>
							<ul>
								<li class="fragment">From own music library</li>
								<li class="fragment">1100 audio cuts (100 files per composer)</li>
								<li class="fragment">30-second 22.05 kHz mono wav clips</li>
							</ul>
						</ul>
					</section>

					<section data-background="#E1D9FC" data-background-transition="slide">
						<h3>Accuracy Results</h3>
						<img src="img/results.png" alt="Results">
					</section>
				</section>

				<section>
					<section data-background="#FFC04C" data-background-transition="slide">
						<h3>Conclusions</h2>
						<ul>
							<li class="fragment roll-in">A different approach of a music audio classification problem from the music theory point of view using a structural analysis of the musical work</li>
							<li class="fragment roll-in">The propagated error of the feature extraction is crucial for this theory to work</li>
							<li class="fragment roll-in">Comparing with the overall results of MIREX 2014, the best result is comparable to BK4</li>
						</ul>

						<aside class="notes">
						</aside>
					</section>
					<section data-background="#FFC04C" data-background-transition="slide">
						<h3>Future work</h3>
							<ul>
									<li class="fragment">Melody or bass patterns (using implementations from the state of the art of MIREX which are close to 90% of accuracy)</li>
									<li class="fragment">Improve the segmentation feature using history ("n-gram segmentation")</li>
									<li class="fragment">More accurate tonality detection to improve the harmony analysis</li>
									<li class="fragment">Or use different machine learning algorithms like Convolutional Neural Nets</li>
									<li class="fragment">Investigate the viability of profiling the composer of the classical works</li>
							</ul>
						<aside class="notes">
						</aside>
					</section>
				</section>

				<section>
					<h1>Thanks</h1>
					<p><i>Code, report and presentation</i></p>
					<p><a href="https://github.com/Lesbinary/tfm/">github.com/Lesbinary/TFM/</a></p>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'linear', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
