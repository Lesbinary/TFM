\documentclass[a4paper,openany,oneside,12pt]{book}
\usepackage[UKenglish]{babel}
\usepackage[UKenglish]{isodate}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning}

% Defines a `datastore' shape for use in DFDs.  This inherits from a
% rectangle and only draws two horizontal lines.
\makeatletter
\pgfdeclareshape{datastore}{
  \inheritsavedanchors[from=rectangle]
  \inheritanchorborder[from=rectangle]
  \inheritanchor[from=rectangle]{center}
  \inheritanchor[from=rectangle]{base}
  \inheritanchor[from=rectangle]{north}
  \inheritanchor[from=rectangle]{north east}
  \inheritanchor[from=rectangle]{east}
  \inheritanchor[from=rectangle]{south east}
  \inheritanchor[from=rectangle]{south}
  \inheritanchor[from=rectangle]{south west}
  \inheritanchor[from=rectangle]{west}
  \inheritanchor[from=rectangle]{north west}
  \backgroundpath{
    %  store lower right in xa/ya and upper right in xb/yb
    \southwest \pgf@xa=\pgf@x \pgf@ya=\pgf@y
    \northeast \pgf@xb=\pgf@x \pgf@yb=\pgf@y
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@ya}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@ya}}
    \pgfpathmoveto{\pgfpoint{\pgf@xa}{\pgf@yb}}
    \pgfpathlineto{\pgfpoint{\pgf@xb}{\pgf@yb}}
 }
}
\makeatother



\newcommand{\textsharp}{$\sharp$}

\begin{document}

%titlepage
\thispagestyle{empty}
\begin{center}
\begin{minipage}{0.75\linewidth}
    \centering
%University logo
    \includegraphics[height=1.5cm]{img/logo-upv}\\
    \vspace{2cm}
%Thesis title
    {\uppercase{\Large Audio Classical Composer Identification in MIREX 2015: Submission based on Structural Analysis of Music\par}}
    \vspace{2.5cm}
%Author's name
    {\Large Leopoldo Pla Sempere\par}
        {\it leoplsem@posgrado.upv.es}\\
        Polytechnic University of Valencia\\
        \vspace{1cm}
        {\uppercase {Departament of Computer Systems and Computation}}\\
        \vspace{0.5cm}
        {\it Supervised by: Roberto Paredes Palacios}\\
        \vspace{0.5cm}
        Master's Degree in Artificial Intelligence, Pattern Recognition and Digital Imaging\\
    \vspace{2cm}
%Degree
    {\Large M.Sc. Thesis\par}
    \vspace{2cm}
%Date
\date{\today}
    {\today}
\end{minipage}
\end{center}
\clearpage



\newpage
\mbox{}
\thispagestyle{empty}




\chapter*{}
\begin{flushright}
\textit{Dedicado a \\
Laura}
\end{flushright}


\newpage
\thispagestyle{empty}
\mbox{}


\chapter*{Abstract}
\addcontentsline{toc}{section}{Abstract}
\markboth{ABSTRACT}{ABSTRACT}

This work is the result of interest in neural networks and classical music, knowledge that I wanted to combine after finishing the studies of the IARFID master and professional music degree. I wanted to contribute to an engineering task from the musicological point of view, so, after working on PAN 2015 author profiling task and learning about sentiment analysis, I concluded that I could use similar techniques in MIREX classical composer identification task.

MIREX is the Music Information Retrieval Evaluation eXchange organized by the University of Illinois at Urbana-Champaign (UIUC) in which every year they prepare several Music Information Retrieval tasks held as part of the International Conference on Music Information Retrieval (ISMIR), which this year is celebrated in Malaga. In this exchange exist the Audio Classification (Train/Test) Tasks, and more specifically the Audio Classical Composer Identification, which is perfect for investigating the use of machine learning and music analysis on classical music.

Machine learning is a field of the artificial intelligence which evolves from the study of pattern recognition and include the study of algorithms that can learn and make decisions over data. Specifically, a neural network is a machine learning algorithm that is inspired by the biological neural networks.

Musical analysis is the process whereby a music piece is decomposed and understood as we analyze a text. There is no exact method for these analysis and differs from analyst to analyst. The lowest level of this musical analysis is the harmonic and functional analysis and a higher one is the structure of the segmentation.

This investigation includes the development of the idea and the software that implements the preprocessing and the classification of samples using different software technology layers, as Sonic Annotator, MATLAB and Bash scripts.


\newpage
\thispagestyle{empty}
\mbox{}



\tableofcontents

\cleardoublepage
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

\cleardoublepage
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables


\newpage
\thispagestyle{empty}
\mbox{}
\setlength{\parskip}{\baselineskip}




\chapter{Introduction}
\pagenumbering{arabic}
In the last years, Music Information Retrieval fields have been improving step by step and the majority of these fields are still using timbral and chroma features of the audio signal, that is, features based on the timbre of the acoustic wave. On the other hand, in computational linguistics authorship attribution is usual to use features from the text representation as character measures, lexical measures and syntactic features. In fact, one of the features that the winner of PAN 2014 (Evaluation lab on uncovering Plagiarism, Authorship, and Social Software Misuse) in the authorship task\cite{pan14} and most of author profiling task submissions in 2013\cite{pan13} use is the part of speech (POS or grammatical) tagging of the text.

So, in this project I approximate the idea of musical structure analysis of classical audio files to identificate composers. This whole concept could be compared to apply POS tagging and segmentation to a speech recording.

Then, in this thesis we will focus on techniques of musical analysis, composition and machine learning instead of signal processing because none of the previous works in the author identification submissions of MIREX used this kind of high level features.

This thesis is distributed in four sections:
\begin{itemize}
\item Introduction, in which we establish the state of the art from the MIREX contest and some theoretical concepts about the elements that are involved on the idea of the thesis, as Artificial Neural Networks and Musical Analysis.
\item Implementation, where I explain the used technologies and the different features that I look for in a sample to identify the composer explaining why I chose them and how they work.
\item Experiments, which explains and shows the tests with their results under a homemade audio dataset.
\item Conclusions, summarizing and expanding the main idea for improvements and useful inferences of the process and results.
\end{itemize}

\section{MIREX}
The Music Information Retrieval Evaluation eXchange (MIREX) edition of 2015 is organized by The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the Graduate School of Library and Information Science (GSLIS), University of Illinois at Urbana-Champaign (UIUC). It's hold as part of the 16th International Conference on Music Information Retrieval, ISMIR 2015, which will be held in Malaga, Spain, October 26th-30th, 2015.

This exchange has several kind of tasks related to Music Information Retrieval:

\begin{itemize}
    \item Grand Challenge on User Experience
    \item Audio Classification (Train/Test) Tasks, incorporating:
    \begin{itemize}
        \item Audio US Pop Genre Classification
        \item Audio Latin Genre Classification
        \item Audio Music Mood Classification
        \item Audio Classical Composer Identification
        \item Audio K-POP Mood Classification
        \item Audio K-POP Genre Classification 
    \end{itemize}
    \item Audio Cover Song Identification
    \item Audio Tag Classification
    \item Audio Music Similarity and Retrieval
    \item Symbolic Melodic Similarity
    \item Audio Onset Detection
    \item Audio Key Detection
    \item Real-time Audio to Score Alignment (a.k.a Score Following)
    \item Query by Singing/Humming
    \item Audio Melody Extraction
    \item Multiple Fundamental Frequency Estimation \& Tracking
    \item Audio Chord Estimation
    \item Query by Tapping
    \item Audio Beat Tracking
    \item Structural Segmentation
    \item Audio Tempo Estimation
    \item Discovery of Repeated Themes \& Sections
    \item Audio Downbeat Estimation
    \item Audio Fingerprinting
    \item Singing Voice Separation 
\end{itemize}

This work is oriented to the Audio Classification (Train/Test) Task of Audio Classical Composer Identification, which submissions must be
\begin{quote}
[...] algorithms to classify music audio according to the composer of the track (drawn from a collection of performances of a variety of classical music genres).[...]
\end{quote}

The authors to classify are:
\begin{itemize}
\item Bach
\item Beethoven
\item Brahms
\item Chopin
\item Dvorak
\item Handel
\item Haydn
\item Mendelssohn
\item Mozart
\item Schubert
\item Vivaldi 
\end{itemize}

\section{State of the Art}
Since this work is focused on MIREX exchange, the state of the art is easily obtainable because all the papers and submissions from previous years are stored and classified in the MIREX Wiki\cite{mirexWiki} and also lots of materials of the ISMIR proceedings\cite{ismirProc}.

For the specific task of Classical Composer Identification in MIREX 2014 the machine learning algorithms used were Convolutional Neural Networks (CNN)\cite{qiuqiang} and multiclass Support Vector Machines (SVM)\cite{seung-ryoel}\cite{shumin}\cite{ming}\cite{byun}, also using PCA\cite{shumin} and a SVM ranker\cite{seung-ryoel} as a dimensional reduction algorithm. Previous years there were used Restricted Boltzmann Machines (RBM)\cite{pikrakis}, Gaussian Mixture Models (GMM)\cite{franz}, k-Nearest Neighbours classifier (kNN) and Multi Layer Perceptron (MLP)\cite{hamel}. In general, SVM is the most used technique for this task.

From the features point of view, the most used features are timbral features as mel-frequency cepstral coefficients (MFCC)\cite{seung-ryoel}\cite{byun}\cite{franz}, decorrelated filter banks (DFB)\cite{seung-ryoel}\cite{byun} and octave-based spectral contrast (OSC)\cite{seung-ryoel}\cite{byun}, visual features as spectograms\cite{seung-ryoel}, spectral patterns (SP)\cite{klaus}, Delta spectral patterns (DSP)\cite{klaus} and Short Time Fourier Transform (STFT)\cite{qiuqiang}\cite{shumin}, and also rhythmic patterns as Logarithmic Fluctuation Patterns (LFP)\cite{klaus} and Correlation Patterns (CP)\cite{klaus}. After the extraction of these features, sometimes are preprocessed before using them in the machine learning system with some statistical features as mean and variance of the timbral features\cite{seung-ryoel} or with Gabor filters\cite{ming12}, as it is usually done in Biometrics and Computer Vision preprocessing stage.

\begin{table}
\centering
   \begin{tabular}{ | l || l | }
   \hline
   Algorithm & Normalised Classification Accuracy \\ \hline
   BK1 & 0.6876 \\ \hline
   SS6 & 0.6732 \\ \hline
   SSKS1 & 0.6728 \\ \hline
   WJ2 & 0.6089 \\ \hline
   QK2 & 0.5685 \\ \hline
   BK2 & 0.5213 \\ \hline
   BK3 & 0.4015 \\ \hline
   BK4 & 0.3947 \\ \hline
   XG3 & 0.3312 \\ \hline
   XG2 & 0.3312 \\ \hline
   \end{tabular}

\caption{Normalised Classification Accuracies of MIREX 2014.\cite{resultsMirex14}} \label{fig:MIREX14results}
\end{table}

Note that most of the submissions for Composer Classification task were also submitted for all Audio Classification (Train/Test) Tasks, so they are relatively generic audio classifiers algorithms for music genres, moods and composers.

Also note that none of the submissions is based on any elements of musical analysis.

\section{Neural Networks}


One of the most famous techniques of machine learning nowadays is the neural network topology.

The basic definition of a machine learning algorithm like the artificial neural network is a process which an object is represented using acquired proprieties from sensors and later is recognized, classified or affects a prediction.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{img/machinelearning.png} 
\caption{Machine learning generic scheme of perception\cite{rparedesNN}} \label{fig:machinelearning}
\end{figure}

An artificial neural network is a class of classification algorithm that belongs to the connectionist models and it is trained to solve problems by learning the features of the problem through a set of already classified data, or labeled training set (supervised learning). This specific kind of software is designed based on the study of how biologic brain works and it has lots of applications in computer vision, speech recognition, natural language processing, bioinformatics and many other science sectors.


\begin{quote}
\em The word network in the term 'artificial neural network' refers to the inter-connections between the neurons in the different layers of each system. An example system has three layers. The first layer has input neurons which send data via synapses to the second layer of neurons, and then via more synapses to the third layer of output neurons. More complex systems will have more layers of neurons, some having increased layers of input neurons and output neurons. The synapses store parameters called "weights" that manipulate the data in the calculations. \ref{fig:neuralnet}

An ANN is typically defined by three types of parameters:
\begin{itemize}
\item The interconnection pattern between the different layers of neurons
\item The learning process for updating the weights of the interconnections
\item The activation function that converts a neuron's weighted input to its output activation.
\cite{wiki:ann}
\end{itemize}
\end{quote}


\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{img/neuralNet.png} 
\caption{General structure of a multilayer perceptron\cite{rparedesNN}} \label{fig:neuralnet}
\end{figure}

Also, this kind of topology is easily paralelizable and it has been implemented in so many frameworks that use several CPU and GPU cores (CUDA or OpenCL, for example).

The origin of the artificial neural networks can be stablished at the first models defined with mathematical notation by McCullock and Pitt in 1943, but the first successful results are obtained in 1959 when the perceptron is created by Rosenblat, which later in 1975 is improved by Paul Werbos adding the backpropagation algorithm.

Some years later the use decreased by the efficiency of Support Vector Machines and some unsupervised learning algorithms due to the difficulty of obtain labeled data.

In the last years, the use of ANN learning algorithms has increased compared to the popular Radial Basis Functions or the Support Vector Machines algorithms, caused by the "boom" of the Deep Learning and the Convolutional Networks.

Nowadays, the most used artificial neural networks are the Restricted Boltzmann Machines, the Convolutional Neural Networks and the Deep Belief Networks.

\subsection{Unsupervised Learning}
As Bishop states in his book "Pattern Recognition and Machine Learning":
\begin{quote}
\em In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization.\cite{Bishop}
\end{quote}

In short, unsupervised learning is a type of learning technique focused on finding hidden structure or drawing inferences from unlabeled input data, as k-Nearest-Neighbours, Gaussian mixture models or Hidden Markov models do. The Restricted Boltzmann machine is a new Deep Learning technique which belongs to the unsupervised learning algorithms and is the main component of Deep Belief Networks.

\subsection{Restricted Boltzmann Machines}
The Restricted Boltzmann Machines are a type of stochastic artificial forward-backward neural network with sigmoid activation functions. They also are a particular form of log-linear Markov Random Field based on the Energy function.

This neural network is composed by a layer of visible units, a layer of hidden units (the latent factors we try to learn) and a bias unit.

An example of use of a Restricted Boltzmann Machine can be seen in \ref{fig:rbm}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{img/rbm.png} 
\caption{RBM used to classify films based on two natural groups: SF-fantasy and Oscar winners\cite{rbmGit}} \label{fig:rbm}
\end{figure}


\subsection{Deep Belief Network}
The Deep Belief Network is a type of Deep Learning algorithms, which is based on a set of stacked Restricted Boltzmann Machines undirectly connected and trained with unsupervised learning, usually the Contrastive Divergence algorithm. These RBMs try to reconstruct the data layer by layer. \ref{fig:dbn} The resultant DBN can be unfolded into a Multi Layer Perceptron (feed forward network) with initialized values and after it can be "fine-tuned" with labeled data using backpropagation.

In the Deep Belief Network
\begin{quote}
\em [...] the top two layers have undirected, symmetric connections between them and form an associative memory. The lower layers receive top-down, directed connections from the layer above. The states of the units in the lowest layer represent a data vector.

The two most significant properties of deep belief nets are:

\begin{itemize}
\item There is an efficient, layer-by-layer procedure for learning the top-down, generative weights that determine how the variables in one layer depend on the variables in the layer above. 

\item After learning, the values of the latent variables in every layer can be inferred by a single, bottom-up pass that starts with an observed data vector in the bottom layer and uses the generative weights in the reverse direction.\cite{Hinton:2009}
\end{itemize}
\end{quote}

Since the final result can be unfolded into a MLP, the main practical difference of using DBN is that this resulting MLP is initialized with the reconstructed data of the DBN instead of a random initialization, which usually obtains better results and can be trained with more unlabeled data that the used for MLP.


\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{img/dbn_model.png} 
\caption{Deep Belief Network training example\cite{rparedesNN}} \label{fig:dbn}
\end{figure}


\subsection{Musical analysis}\label{musical_analysis}
The process and the academic discipline which studies the musical works from the pattern aspect, the internal structure, composition techniques and interpretative aspects is the musical analysis.

The basic objectives of the musical analysis are the recognition of different elements of the work:

\begin{itemize}
\item Structure
\item Melody
\item Texture
\item Harmony
\item Tone
\item Dynamic
\end{itemize}

Also, is objective of study the existent relationships inside the previous elements and between them to finally obtain conclusions about the relationships, the compositor ideas and why the composer structured the elements in that way.

Usually, the musical analysis is performed over a score because is easier to identify these mentioned elements, but also listening to some parts of the work is complementary.

The musical analysis includes different subanalysis or techniques that are used to describe the musical work:

\begin{itemize}
\item Discretization: breaking the piece down into smaller parts and examine the way these parts are connected. In fact, Fred Lerdahl\cite{Lerdahl} argues that discretization is indispensable for a musical analysis.
\item Harmonic analysis: indicate the harmonic functions on the same score including figured bass and modulations if harmony is tonal.
\end{itemize}

Although the discretization is more intuitive to perform, the harmonic analysis implies the knowledge of the harmonies in different scales\ref{fig:scales}. Also, the knowledge of the most famous cadences is very important. These are some of them:

\begin{itemize}
\item Perfect Authentic Cadence (PAC): V to I (chords in root position)
\item Imperfect Authentic Cadence (IAC): V to I (with modified notes)
\item Half Cadence (HC): V of V, ii, vi, IV, or I to V
\item Plagal Cadence (PC): IV to I
\item Interrupted Cadence: V to vi
\end{itemize}

And there are more cadences and larger common progressions, like "I IV I IV" or "I IV V7 I" that are interesting to obtain from the analyzed work \cite{jacmuse}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{img/scales.png} 
\caption{Figured bass in different scales of C} \label{fig:scales}
\end{figure}

As an illustrative example of a music analysis, looking at this Beethoven Minuet\ref{fig:music_analysis} we can see the letters above the staff that denotes the different sections of the score. We obtained "a", "b", "c" and again "b". Also, we made the harmonic analysis ("I", "V", "IV" and the other variants, inversions and elements below the staff) focusing on cadences like Half Cadence (HC), Perfect Authentic Cadence (PAC) or Imperfect Authentic Cadence (IAC). There are more elements like passing tones (PT) and subjective descriptions at the margins.

But the interesting idea here is that the previous features, like in the texts, help to discriminate the authors.

This field of the music theory covers music from medieval era to the contemporary music, but in this thesis we are focused on classical music analysis and the application of the techniques that we showed in the example.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{img/musical_analysis.jpg} 
\caption{Analysis of a Beethoven Minuet No. 3. of 4 in E-flat} \label{fig:music_analysis}
\end{figure}

\newpage
\thispagestyle{empty}
\mbox{}



\chapter{Implementation}\label{implementation}
In this section I explain each part of the proposed system \ref{fig:workflow} from the technologies I chose and the features that I extracted to identify the composers.

\section{Technologies}\label{techs}
These are the technologies used in all the stages of the system.

\subsection{Sonic Annotator and Visualizer}
Sonic Annotator\cite{chris2010a} is a batch tool for feature extraction and annotation of audio files using Vamp plugins (binary modules that extract descriptive information from audio data). It was originally developed in 2009 at Queen Mary, University of London as part of the OMRAS2 project, to facilitate feature data publication on the Semantic Web. It has been used in audio annotation projects and it's used to power online audio feature retrieval APIs. It's published under GNU General Public License v2.

Sonic Visualizer\cite{SonicVisualiser} is a graphical application that plots the content of the extracted data from Sonic Annotator. It's also developed by the Centre for Digital Music at Queen Mary, University of London and published under GPLv2.

Both tools are powerful because they allow you to use the QM Vamp Plugins that were presented to MIREX in previous years\cite{vamp} and they have an approximate accuracy of 80\% in Chord Detection and Key Estimation \cite{noland}.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{img/semantic-web-and-friends-webscale.png} 
\caption{Workflow diagram of Sonic Annotator and Visualizer} \label{fig:diag_sonic}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{img/sonic_visualizer.png} 
\caption{Fragment of Bach's Prelude no. 568 with Key Strength Plot in Sonic Visualizer} \label{fig:keystrength}
\end{figure}

\subsection{Python}
High level programming language designed to be easy to read, understand and implement. It supports multiple programming paradigms including object-oriented, imperative, functional and procedural. It's published under PSFL licence (open source).

I chose this programming language because it makes easy to work with CSV files and dictionaries (the C++ or Java "hashmaps") for counting the feature values of the audio samples.

\subsection{Bash}
Bash is a Unix shell and a command language written by Brian Fox for the GNU Project as a free replacement of the Bourne shell. Bash Shell is included in most of GNU/Linux and OS X releases. 

The most famous features of Bash (and all Unix shells) are filename globbing (wildcard matching), piping, command substitution, variables and control structures for condition-testing and iteration.

With this features, this tool is perfect to simplify the way of working with thousand of files that need to be passed to other languages scripts or programs, like Python or Sonic Annotator. Also, the example of submission calling formats showed ".sh" scripts for feature extraction, training and classifying samples, so I followed it.

\subsection{MATLAB and Octave}
MATLAB is a multi-paradigm numerical computing environment and a proprietary programming language developed by MathWorks. This environment is focused on matrix manipulations and functions and data plotting. For this reasons, this environment (along with Python) is the most used in Mathematics and Statistical fields.

Octave is quite similar to MATLAB and most programs are compatible with both, but Octave is developed as open source and part of GNU Project.


\subsection{Deep Learn Toolbox}
The Deep Learning Toolbox of Rasmus Berg is a Matlab/Octave toolbox for Deep Learning. Includes Deep Belief Nets, Stacked Autoencoders, Convolutional Neural Nets, Convolutional Autoencoders and vanilla Neural Nets. It's very easy to use, documented and gives working examples in actual problems giving state of the art results.\cite{IMM2012-06284}

Also, given the practice I got in IARFID master using this toolbox in several subjects and the good results and performance it showed, I wanted to save some time from learning more tools using it again.



\section{Feature extraction}\label{sec:feature_extraction}
In this chapter I explain the features I extracted from audio samples in the training process (at the beginning) to train the neural network and at the end, at the moment to classify new samples.


\subsection{Key Mode}\label{subsec:keymode}
The Vamp Plugin "Key Mode" calculates the major or minor mode of the estimated key in windows of 10 chroma frames. After calculate them, I use the count of the changes between minor and major as a feature. Major is written as 0 and minor as 1.

\subsection{Segmentation}\label{subsec:segmentation}
This is also feature from a Vamp Plugin from Queen Mary which divides the channel into 10 structural segments based on chroma and MFCC. Also, it labels similar segments, which gives us a structural analysis of the sample based on tonality. As key mode, I also use the count of the segments that appear at the segment.\ref{fig:sonic}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{img/segmentation.png} 
\caption{Fragment of Bach's Prelude no. 568 with Segmentation plot in Sonic Visualizer} \label{fig:sonic}
\end{figure}

\subsection{Tonality}\label{subsec:chord_windows}
The main work has been done in these following high level features based on the key of the sample and the detected chords. First of all, I obtain through the key strength Vamp plugin\ref{fig:keystrength} the value between -1 and 1 of every key (from C, C\#, and so on, to B minor and major) of every window of 1 chroma frame. Then, I select the most strong key of every window (in the figure, the most red value of a column) obtaining a vector of keys.

From this keys vector, I created a script in Python \ref{fig:keyFormula} that obtains the key of the sample using a weighted sum of the number of perfect cadences found at the key strength using 1 chroma and the values of key strength plugin using 10 chroma. In the mathematical notation, keys are sorted by 12 Major tones and then 12 minor tones, the ponderation in the implementation is 12 after some tests and the w is the 1 and 10 chroma window through the algorithm iterates. This obtained key is used as a feature.

\begin{figure}

\[ Tone(Keys,pond) = argmax_{k \in Keys} \sum_{w_{1}=1}^{n_{win}} w_k \sum_{w_{10}=2}^{n_{win}} w_k * (1-w_{(k+7)\%12})*pond \]
\caption{Formula to obtain a ponderated key using perfect cadences and key strength} \label{fig:keyFormula}
\end{figure}


\subsection{Harmonic analysis}\label{subsec:uni_bi_tri}
The previous key vector, then, need to be transposed to convert this incontextual chords into a functional chords to get a functional harmonic analysis of the sample. To transpose, I use the previous obtained key of the sample.

From the previous feature we can obtain the number of functional units in the sample (tonic, dominant, subdominant, etc.), the number of most used cadences (perfect authentic cadence, plagal cadence, half-cadence, etc.) and even a set of most used progressions of three chords (like IV-V-I) \ref{musical_analysis}. As a parallelism of the POS tagging in text, is interesting to analyze the impact of this features because it's known that discriminate the composers \cite{desportes}.

\subsection{MFCC means}\label{subsec:mfcc_means}
At last, I included the means of the MFCC (Vamp Plugin) from the sample, using 20 coefficients and including C0, also to compare this timbral feature with the structural ones.


\section{Classification}\label{sec:classification}
In the proposed systems, I used a Multi Layer Perceptron (Feedforward Backpropagation Neural Network or simply NN) in one system and a Deep Belief Network (DBN) in another one as a classifiers. The features are normalized with z-score to remove outlayers and after that I normalized between 0 and 1 before using the neural network because of artificial neural network input restriction by design. The networks are configured with 44 neurons at the hidden layer, 300 epochs, a batch size based on a divisor of the number of dataset samples, sigmoid activation function and softmax function at the output layer. I tested the system with a homemade database of FLAC files, extracted from my own CD's of the authors of the task.

\begin{figure}
\begin{center}
\begin{tikzpicture}[
  font=\sffamily,
  every matrix/.style={ampersand replacement=\&,column sep=2cm,row sep=2cm},
  source/.style={draw,thick,rounded corners,inner sep=.3cm},
  process/.style={draw,thick,circle},
  sink/.style={source},
  datastore/.style={draw,very thick,shape=datastore,inner sep=.3cm},
  dots/.style={gray,scale=2},
  to/.style={->,>=stealth',shorten >=1pt,semithick,font=\sffamily\footnotesize},
  every node/.style={align=center}]

  % Position the nodes using a matrix layout
    \node[source] (dataset) {Dataset};
      \node[process, below left = of dataset] (bps) {Bash\\and\\Python\\scripts};
      \node[datastore, below = of bps] (Tfeatures) {training features\\into\\scratch folder};
      \node[process, below = of Tfeatures] (NN) {Matlab\\NN};
      \node[datastore, below = of NN] (trained_nn) {Trained NN};
      \node[process, right = of NN] (bcs) {Bash\\class.\\script};
      \node[datastore, below = of bcs] (Cfeatures) {classification\\features};
      \node[process, below = of Cfeatures] (mat_class) {Matlab\\Class.};
      \node[sink, below = of mat_class] (classification) {Classes of files};


  % Draw the arrows between the nodes and label them.
  \draw[to] (dataset) -- node[midway,left] {load training files} (bps);
  \draw[to] (bps) -- node[midway,right] {using\\Sonic\\Annotator} (Tfeatures);
  \draw[to] (Tfeatures) -- node[midway,right] {used to\\train} (NN);
  \draw[to] (NN) -- node[midway,left] {once converge} (trained_nn);
  \draw[to] (dataset) -- node[midway,right] {load test files} (bcs);
  \draw[to] (trained_nn) -- node[midway,left] {is loaded in} (mat_class);
  \draw[to] (bcs) -- node[midway,right] {obtains} (Cfeatures);
  \draw[to] (Cfeatures) -- node[midway,right] {are passed to} (mat_class);
  \draw[to] (mat_class) -- node[midway,right] {obtains} (classification);
\end{tikzpicture}
\caption{Workflow of the proposed system}\label{fig:workflow}
\end{center}
\end{figure}

\newpage
\thispagestyle{empty}
\mbox{}


\chapter{Experiments}\label{experiments}
For the experiments, I used a database created by myself. The reason for this decision is that music databases are not easy to distribute because of copyright issues and this task requires a very specific audio samples.

The database I created is structured exactly the same way that the MIREX task specifies:

\begin{quote}
\begin{itemize}
\item 2772 30-second 22.05 kHz mono wav clips
\item 11 "classical" composers (252 clips per composer), including:
	\begin{itemize}
	\item Bach
	\item Beethoven
	\item Brahms
	\item Chopin
	\item Dvorak
	\item Handel
	\item Haydn
	\item Mendelssohn
	\item Mozart
	\item Schubert
	\item Vivaldi 
	\end{itemize}
\end{itemize}
\end{quote}

The disks I used are (respectively to each composer):
\begin{itemize}
	\item Bach - The Baroque Music Library (\url{http://www.baroquemusiclibrary.com/})
	\item The Very Best of Beethoven (ASIN: B000B6N64A)
	\item The Best of Brahms (ASIN: B0000014HA)
	\item Chopin - Complete Edition (Deutsche Grammophon) (ASIN: B00001X58Z)
	\item The Very Best of Dvorak (ASIN: B000S5C8O8)
	\item Handel - The Baroque Music Library  (\url{http://www.baroquemusiclibrary.com/})
	\item Haydn - The Baroque Music Library (\url{http://www.baroquemusiclibrary.com/})
	\item Mendelssohn - The complete symphonies (ASIN: B003Y3MYWC)
	\item Mozart - Complete Works (ASIN: B000BLI3K2)
	\item Franz Schubert - Masterworks (ASIN: B00062FLJC)
	\item The Very Best of Vivaldi (ASIN: B000B6N63Q)
\end{itemize}


After downloading (most of them in MP3) and dumping the audios into WAV files using Asunder app, I created a "quick" bash command to convert all .mp3 files into WAV:
\lstset{language=Bash,
           basicstyle=\ttfamily\scriptsize,
           keywordstyle=\ttfamily,
           stringstyle=\ttfamily,
           commentstyle=\ttfamily,
          breaklines=true
          }
\begin{lstlisting}
find * -type f -name "*.mp3"  -exec avconv -i '{}' '{}.wav'
\end{lstlisting}

And also another one to cut all files into 30 seconds fragments and classified in folders by composer:
\lstset{language=Bash,
           basicstyle=\ttfamily\scriptsize,
           keywordstyle=\ttfamily,
           stringstyle=\ttfamily,
           commentstyle=\ttfamily,
          breaklines=true
          }
\begin{lstlisting}
for FOLDER in `ls`; do cd $FOLDER; rename 's/^(.{30}).*(\..*)$/$1$2/' * && find | rename 's/[\ \-(),;]//g' && COUNTER=1; for FILE in `ls`; do LENGTH=`nohup sox $FILE -n stat | grep "Length (seconds):" | sed 's/\ //g' | cut -f 2 -d ':' | cut -f 1 -d '.'`; LENGTH=$(( LENGTH - 30 )); LENGTH=${LENGTH/#-/}; for i in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20; do STARTVALUE=`echo $RANDOM % $LENGTH + 1 | bc`; COMMAND="sox "$FILE" "${COUNTER}_$FILE" trim $STARTVALUE 30"; COUNTER=$((COUNTER+1)); echo $COMMAND; eval $COMMAND; done ; rm $FILE; done; cd ..; done
\end{lstlisting}

Also, to test the workflow of the MIREX task, I created the list of files for feature extraction, training and classification with another simple Bash script:

\lstset{language=Bash,
           basicstyle=\ttfamily\scriptsize,
           keywordstyle=\ttfamily,
           stringstyle=\ttfamily,
           commentstyle=\ttfamily,
          breaklines=true
          }
\begin{lstlisting}
find /home/leopoldo/tfm/audio/ -type f > featureExtractionFiles.txt
cat featureExtractionFiles.txt | cut -f 6 -d '/' > tmp
paste featureExtractionFiles.txt tmp > trainingFiles.txt
find /home/leopoldo/tfm/audio-testing/ -type f > classifyFiles.txt
cat classifyFiles.txt | cut -f 6 -d '/' > tmp
paste classifyFiles.txt tmp > expectedResult.txt
rm tmp
\end{lstlisting}


This created an amount of 2420 files (6.4GB), but due to the length gap between authors after some tests, I decided to delete some audio fragments from some composers and created a final corpus of 1100 files (2.100 files per author).

So, I experimented with my dataset focusing on the harmonic analysis because that features were implemented by myself. I experimented with different Neural Network configurations, with DBN and NN, and using different groups of features to compare the impact of each of them using a three-fold cross validation process.

I named "unigrams" and "bigrams" the figured bass and the cadences given the simile with text n-grams.


\begin{table}
\centering

   \begin{tabular}{ | l | l || l | l |}
   \hline
   Features & \# of features & DBN & NN \\ \hline
   Means MFCC & 20 & 33.5 & \bf{37.1} \\ \hline
   Means MFCC + Segmentation + Major/Minor & 32 & \bf{27.8} & 25.9 \\ \hline
   Harmonic Analysis + Means MFCC & 68 & 28.0 & \bf{35.3}\\ \hline
   Harmonic Analysis & 48 & \bf{12.1} & 10.5 \\ \hline
   Unigrams+Bigrams + Means MFCC & 45 & 32.3 & \bf{33.6} \\ \hline
   Unigrams+Bigrams & 25 & \bf{10.1} & 9.4 \\ \hline
   Unigrams + Means MFCC & 32 & 27.8 & \bf{38.0} \\ \hline
   Unigrams & 12 & 8.9 & \bf{9.5} \\ \hline
   Segmentation & 10 & 8.4 & \bf{8.7} \\ \hline
   Harmonic Analysis + Segmentation + Major/Minor & 60 & \bf{11.3} & 11.1 \\ \hline
   All & 80 & 18.1 & \bf{29.7} \\ \hline
   \end{tabular}

\caption{Average three-fold cross-validation accuracies, in percent. Moldface: best performance for a given setting (row).} \label{fig:results}
\end{table}


NOTE: The results of the MIREX Train/Test tasks are not available at the submission deadline of this report.

\newpage
\thispagestyle{empty}
\mbox{}


\chapter{Conclusions}\label{conclusions}
In this work we explained a different approach of a music audio classification problem from the music theory point of view using a structural analysis of the musical work. After the results, we can conclude that the propagated error of the feature extraction is crucial for this theory to work. This can be seen at the penultimate test, which uses all the features I implemented and the accuracy is the more or less the same as a "random" classifier, and the MFCC means based classifier gets more than almost all the other results. But we can see little improvements if we compute the classification with figured bass (unigrams).

Also, if we compare with the overall results of MIREX 2014 \ref{fig:MIREX14results}, the best result is comparable to BK4, which used MFCC, DFB, OSC, Gabor filters, temporal and visual features in a SVM machine\cite{seung-ryoel}.

 

\section{Future work lines}
Given this initial idea of working with audio from the harmony and structure of the music, this idea can be improved looking for more plausible patterns to identify:

\begin{itemize}
\item Melody or bass patterns (using implementations from the state of the art of MIREX which are close to 90\% of accuracy)
\item Improve the segmentation feature using history ("n-gram segmentation")
\item Other timbral features to compare with (like DFB or OSC)
\item More accurate tonality detection to improve the harmony analysis
\item Or use different machine learning algorithms like Convolutional Neural Nets
\end{itemize}

And finally, as this work resulted as a parallelism with PAN contest, this thesis idea can be suggested to be used to investigate the viability of profiling the composer of the classical works to obtain the age of the composer (when the work was composed) or even nationalities due to the different schools of European composers, that can be settle a new interesting task in the MIREX exchange.




\nocite{*}
\cleardoublepage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{ieeetr}
\bibliography{bibliography}




\newpage
\thispagestyle{empty}
\mbox{}


\appendix
\chapter{Donwload links}\label{links}
\begin{itemize}

\item Download this document: \url{https://github.com/Lesbinary/TFM/raw/master/doc/report.pdf}

\item Source code download (documentation and implementation): \url{https://github.com/Lesbinary/tfm/archive/master.zip}

\item Github Repository: \url{https://github.com/Lesbinary/tfm}

\item GPLv3 License: \url{https://github.com/Lesbinary/tfm/blob/master/LICENSE}

\end{itemize}


\chapter{Environment configuration}\label{configuration}

This project has been developed under clean install of Ubuntu 15.04 but it also works on Ubuntu 14.04 LTS. The only needed software that is not installed by default is MATLAB and I used version 2014a.

First of all, you need to download my repository (see appendix \ref{links}) into the folder you want. After that, you need to copy the "vamp" folder of the root of the repository into your home folder (\$HOME/vamp) or into /usr/local/lib/vamp because Sonic Annotator searches for plugins in that paths.

Then you are ready to use the script based on the standard of MIREX submissions (\url{http://www.music-ir.org/mirex/wiki/2015:Audio_Classification_%28Train/Test%29_Tasks#Example_submission_calling_formats}), for example:
   \lstset{language=Bash,
           basicstyle=\ttfamily\scriptsize,
           keywordstyle=\ttfamily,
           stringstyle=\ttfamily,
           commentstyle=\ttfamily,
          breaklines=true
          }
\begin{lstlisting}
	tfmFeatureExtractor.sh -numThreads 8 ~/tfm/scratch/ ~/tfm/trunk/featureExtractionFiles.txt
	tfmTrainer.sh -numThreads 8 ~/tfm/scratch/ ~/tfm/trunk/trainingFiles.txt
	tfmClassifier.sh -numThreads 8 ~/tfm/scratch/ ~/tfm/trunk/classifyFiles.txt  ~/tfm/trunk/finalResult
\end{lstlisting}
	
And that's it. Also keep in mind that you will need a system with at least 80MB. It's recommended 100-150MB for using a corpus like the one in MIREX. Also keep in mind that there is an option of multithreading which can improve the speed of the run. The default threads value is 4 and can be set with the parameter "-numThreads" (see previous example).

\end{document}
\grid
